## 1. What NUMA is (clean definition)

**NUMA = Non-Uniform Memory Access**

> In a NUMA system, memory access time depends on **which CPU socket** accesses **which physical memory**.

* Each CPU socket has **local memory**
* Accessing **local memory is fast**
* Accessing **remote memory (another socket‚Äôs RAM) is slower**

**Uniform Memory Access (UMA)** ‚Üí all memory same latency
**NUMA** ‚Üí memory latency is *non-uniform*

---

## 2. Why NUMA exists (hardware reality)

Modern servers:

* Multiple CPU sockets
* Each socket has:

  * Its own memory controller
  * Its own RAM channels
* Sockets connected via:

  * Intel UPI
  * AMD Infinity Fabric

So:

* CPU 0 ‚Üí RAM 0 = fast
* CPU 0 ‚Üí RAM 1 = **cross-socket hop** = slower

üí° This is **physics**, not OS design.

---

## 3. Latency numbers (interview gold)

Typical order of magnitude:

| Access type         | Latency      |
| ------------------- | ------------ |
| L1 cache            | ~1 ns        |
| L2                  | ~3‚Äì5 ns      |
| L3                  | ~10‚Äì15 ns    |
| **Local NUMA RAM**  | ~80‚Äì100 ns   |
| **Remote NUMA RAM** | ~130‚Äì200+ ns |

> Remote NUMA access can be **1.5‚Äì2√ó slower**.

For HFT, that‚Äôs catastrophic.

---

## 4. Why NUMA matters in HFT (this is the punchline)

HFT is:

* Microseconds ‚Üí nanoseconds
* Predictability > throughput

NUMA problems cause:

* Latency spikes
* Jitter
* Unexplained tail latency
* ‚ÄúFast most of the time, slow sometimes‚Äù bugs

If your hot thread:

* Runs on socket 0
* Reads memory allocated on socket 1

‚Üí You lose.

---

## 5. How NUMA bites you (common failure modes)

### 1Ô∏è‚É£ OS default behavior

* Linux allocates memory on **first touch**
* If thread migrates later ‚Üí memory stays put
* Suddenly all accesses are remote

### 2Ô∏è‚É£ Thread migration

* Scheduler moves threads across sockets
* Cache + memory locality destroyed

### 3Ô∏è‚É£ Shared data structures

* One socket writes
* Another socket reads
* Cache line ping-pong across sockets

---

## 6. First-touch policy (VERY important)

Linux uses **first-touch NUMA allocation**:

> Memory is allocated on the NUMA node of the CPU that **first writes to it**.

Implication:

* Initialize memory on the same core that will use it
* Don‚Äôt allocate on one thread and consume on another (unless pinned)

Interview line:

> ‚ÄúAllocation locality is determined by first touch, not malloc.‚Äù

---

## 7. How HFT systems handle NUMA

### CPU pinning

* Pin threads to specific cores
* Never migrate

```bash
taskset
pthread_setaffinity_np
```

### Memory pinning

* Allocate memory on specific NUMA node

```bash
numactl --membind=0 --cpubind=0
```

or via `libnuma`:

```c
numa_alloc_onnode()
```

### One socket = one strategy

* Each strategy isolated per NUMA node
* No cross-socket sharing
* Replicate read-only data

---

## 8. NUMA vs cache coherence (they love this connection)

NUMA ‚â† cache coherence, but they interact.

* Multi-socket systems use **MESI-like protocols**
* Cache lines move across sockets
* Writes cause invalidations
* Remote cache line ownership = latency spikes

Bad pattern:

```cpp
std::atomic<uint64_t> shared_counter;
```

Accessed by threads on different sockets ‚Üí disaster.

---

## 9. NUMA-friendly design patterns

### ‚úÖ Good

* Per-thread data
* Per-socket sharding
* Lock-free, socket-local queues
* Read-only replication

### ‚ùå Bad

* Global locks
* Shared counters
* Centralized allocators
* Cross-socket ring buffers

---

## 10. How to detect NUMA problems (interview flex)

* `numactl --hardware`
* `numastat`
* `perf c2c`
* Latency histograms (p99/p999 spikes)
* Sudden slowdowns after thread migration

---

## 11. One-liners to memorize (drop these casually)

* ‚ÄúNUMA issues show up as tail latency, not average latency.‚Äù
* ‚ÄúLocal memory access is critical for predictable performance.‚Äù
* ‚ÄúFirst-touch allocation determines NUMA locality.‚Äù
* ‚ÄúThread pinning without memory pinning is incomplete.‚Äù
* ‚ÄúCross-socket cache line bouncing is a silent killer.‚Äù

---

## 12. If interviewer asks: ‚ÄúExplain NUMA in 30 seconds‚Äù

Say this:

> ‚ÄúNUMA means memory access latency depends on which CPU socket owns the memory. Local memory is fast, remote memory is significantly slower. In low-latency systems like HFT, we pin threads and allocate memory on the same NUMA node to avoid cross-socket access and unpredictable tail latency.‚Äù

That answer alone passes.
